{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":128049,"databundleVersionId":15406965,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SDRF Extraction — Harmonizing the Data of your Data\n\nExtract SDRF metadata from test papers using the competition baseline prompt and OpenAI, then write `submission.csv`.\n\n### Secrets (for OpenAI)\nIn the right panel: **Add-ons → Secrets**. Add:\n- **`OPENAI_API_KEY`** — your OpenAI API key (required for real extraction)\n- **`OPENAI_MODEL`** (optional) — e.g. `gpt-4o-mini` or `gpt-4o`\n\nWithout `OPENAI_API_KEY`, the run uses **placeholder** mode (all \"Not Applicable\").","metadata":{}},{"cell_type":"code","source":"# Kaggle environment: list input files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q openai","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Config & paths","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom openai import OpenAI\n\n# Load Kaggle Secrets into environment\ntry:\n    from kaggle_secrets import UserSecretsClient\n    client = UserSecretsClient()\n    for name in (\"OPENAI_API_KEY\", \"OPENAI_MODEL\", \"USE_BATCH\", \"LLM_PROVIDER\"):\n        try:\n            os.environ[name] = client.get_secret(name)\n        except Exception:\n            pass\nexcept ImportError:\n    pass\n\n# Paths\nINPUT_DIR = Path(\"/kaggle/input/harmonizing-the-data-of-your-data\")\nWORKING_DIR = Path(\"/kaggle/working\")\n\nTEST_PUBTEXT = INPUT_DIR / \"Test PubText\" / \"Test PubText\"\nif not TEST_PUBTEXT.exists():\n    TEST_PUBTEXT = INPUT_DIR / \"Test_PubText\" / \"Test_PubText\"\nif not TEST_PUBTEXT.exists():\n    TEST_PUBTEXT = INPUT_DIR / \"Test PubText\"\n\nSAMPLE_SUBMISSION = INPUT_DIR / \"SampleSubmission.csv\"\nBASELINE_PROMPT_PATH = INPUT_DIR / \"BaselinePrompt.txt\"\nSUBMISSION_OUT = WORKING_DIR / \"submission.csv\"\n\nMANUSCRIPT_KEYS = (\"TITLE\", \"ABSTRACT\", \"METHODS\")\nMANUSCRIPT_MAX_CHARS = 120_000\nPREDICTION_COLUMNS_EXCLUDE = (\"ID\", \"PXD\", \"Raw Data File\", \"Usage\")\n\nprint(\"Input:\", INPUT_DIR)\nprint(\"SampleSubmission:\", SAMPLE_SUBMISSION.exists())\nprint(\"BaselinePrompt:\", BASELINE_PROMPT_PATH.exists())\nprint(\"Test PubText:\", TEST_PUBTEXT.exists())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 3. Helper functions","metadata":{}},{"cell_type":"code","source":"def strip_json(text):\n    if \"```\" not in text:\n        return text\n    for p in text.split(\"```\"):\n        p = p.strip()\n        if p.lower().startswith(\"json\"):\n            p = p[4:].strip()\n        if p.startswith(\"{\"):\n            return p\n    return text\n\n\ndef parse_llm_response(raw_response, raw_files):\n    text = strip_json(raw_response)\n    try:\n        out = json.loads(text)\n    except json.JSONDecodeError:\n        return {raw: {} for raw in raw_files}\n    for raw in out:\n        for k, v in list(out[raw].items()):\n            if isinstance(v, str):\n                out[raw][k] = [v]\n    return out\n\n\ndef extract_openai(manuscript_text, raw_files, prompt_spec, expected_columns, model=\"gpt-4o-mini\"):\n    api_key = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n    if not api_key:\n        return {raw: {} for raw in raw_files}\n    text = (manuscript_text or \"\")[:MANUSCRIPT_MAX_CHARS]\n    user = f\"MANUSCRIPT_TEXT:\\n{text}\\n\\nRAW_FILES:\\n\" + \"\\n\".join(raw_files)\n    if expected_columns:\n        user += \"\\n\\nUse these exact column names as JSON keys when applicable: \" + \", \".join(expected_columns)\n    client = OpenAI(api_key=api_key)\n    resp = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": prompt_spec},\n            {\"role\": \"user\", \"content\": user},\n        ],\n        temperature=0,\n    )\n    raw = (resp.choices[0].message.content or \"\").strip()\n    return parse_llm_response(raw, raw_files)\n\n\ndef get_manuscript(doc):\n    return \"\\n\\n\".join(doc.get(k, \"\").strip() for k in MANUSCRIPT_KEYS if doc.get(k))\n\n\ndef sdrf_to_row(raw_file, sdrf_per_file, pred_columns):\n    meta = sdrf_per_file.get(raw_file, {})\n    row = {}\n    for col in pred_columns:\n        vals = meta.get(col)\n        if vals and len(vals) > 0:\n            row[col] = vals[0] if isinstance(vals, list) else str(vals)\n        else:\n            row[col] = \"Not Applicable\"\n    return row","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 4. Load template & baseline prompt","metadata":{}},{"cell_type":"code","source":"prompt_spec = \"\"\nif BASELINE_PROMPT_PATH.exists():\n    prompt_spec = BASELINE_PROMPT_PATH.read_text(encoding=\"utf-8\")\nelse:\n    print(\"Warning: BaselinePrompt.txt not found\")\n\nsub = pd.read_csv(SAMPLE_SUBMISSION, index_col=0)\npred_columns = [c for c in sub.columns if c not in PREDICTION_COLUMNS_EXCLUDE]\nn_pxds = sub[\"PXD\"].nunique()\n\nuse_openai = bool(os.environ.get(\"OPENAI_API_KEY\", \"\").strip()) and prompt_spec\nmodel = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\").strip()\n\nprint(f\"Mode: {'OpenAI (' + model + ')' if use_openai else 'Placeholder'}\")\nprint(f\"Template: {len(sub)} rows, {n_pxds} PXDs\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 5. Extract per PXD & build submission","metadata":{}},{"cell_type":"code","source":"out_df = sub.copy()\n\nfor i, (pxd, group) in enumerate(sub.groupby(\"PXD\"), start=1):\n    raw_files = group[\"Raw Data File\"].unique().tolist()\n    path = TEST_PUBTEXT / f\"{pxd}_PubText.json\"\n    if not path.exists():\n        manuscript_text = \"\"\n        print(f\"[{i}/{n_pxds}] {pxd} — no PubText\")\n    else:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            doc = json.load(f)\n        manuscript_text = get_manuscript(doc)\n        if \"Raw Data Files\" in doc:\n            raw_files = doc[\"Raw Data Files\"]\n    print(f\"[{i}/{n_pxds}] {pxd} ...\", end=\" \", flush=True)\n    sdrf_per_file = extract_openai(manuscript_text, raw_files, prompt_spec, pred_columns, model)\n    for idx, r in group.iterrows():\n        row_vals = sdrf_to_row(r[\"Raw Data File\"], sdrf_per_file, pred_columns)\n        for col in pred_columns:\n            out_df.at[idx, col] = row_vals[col]\n    print(\"ok\")\n\nout_df.to_csv(SUBMISSION_OUT, index=True)\nprint(f\"\\nWrote {SUBMISSION_OUT} ({len(out_df)} rows)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## 6. Submit to competition\n\nAfter **Save & Run All** completes, use **Submit** (top right) to send this notebook's output to the leaderboard. The file `submission.csv` will be in the output.","metadata":{}}]}