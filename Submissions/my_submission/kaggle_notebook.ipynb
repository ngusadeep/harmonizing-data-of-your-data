{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SDRF Extraction — Harmonizing the Data of your Data\n",
        "\n",
        "Extract SDRF metadata from test papers using the competition baseline prompt and OpenAI, then write `submission.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "### Secrets (for OpenAI)\n",
        "In the right panel: **Add-ons → Secrets**. Add:\n",
        "- **`OPENAI_API_KEY`** — your OpenAI API key (required for real extraction)\n",
        "- **`OPENAI_MODEL`** (optional) — e.g. `gpt-4o-mini` or `gpt-4o`\n",
        "\n",
        "Without `OPENAI_API_KEY`, the run uses **placeholder** mode (all \"Not Applicable\")."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Kaggle environment: list input files\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q openai"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Config & paths"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load Kaggle Secrets into environment\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    client = UserSecretsClient()\n",
        "    for name in (\"OPENAI_API_KEY\", \"OPENAI_MODEL\", \"USE_BATCH\", \"LLM_PROVIDER\"):\n",
        "        try:\n",
        "            os.environ[name] = client.get_secret(name)\n",
        "        except Exception:\n",
        "            pass\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Paths\n",
        "INPUT_DIR = Path(\"/kaggle/input/harmonizing-the-data-of-your-data\")\n",
        "WORKING_DIR = Path(\"/kaggle/working\")\n",
        "\n",
        "TEST_PUBTEXT = INPUT_DIR / \"Test PubText\" / \"Test PubText\"\n",
        "if not TEST_PUBTEXT.exists():\n",
        "    TEST_PUBTEXT = INPUT_DIR / \"Test_PubText\" / \"Test_PubText\"\n",
        "if not TEST_PUBTEXT.exists():\n",
        "    TEST_PUBTEXT = INPUT_DIR / \"Test PubText\"\n",
        "\n",
        "SAMPLE_SUBMISSION = INPUT_DIR / \"SampleSubmission.csv\"\n",
        "BASELINE_PROMPT_PATH = INPUT_DIR / \"BaselinePrompt.txt\"\n",
        "SUBMISSION_OUT = WORKING_DIR / \"submission.csv\"\n",
        "\n",
        "MANUSCRIPT_KEYS = (\"TITLE\", \"ABSTRACT\", \"METHODS\")\n",
        "MANUSCRIPT_MAX_CHARS = 120_000\n",
        "PREDICTION_COLUMNS_EXCLUDE = (\"ID\", \"PXD\", \"Raw Data File\", \"Usage\")\n",
        "\n",
        "print(\"Input:\", INPUT_DIR)\n",
        "print(\"SampleSubmission:\", SAMPLE_SUBMISSION.exists())\n",
        "print(\"BaselinePrompt:\", BASELINE_PROMPT_PATH.exists())\n",
        "print(\"Test PubText:\", TEST_PUBTEXT.exists())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Helper functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def strip_json(text):\n",
        "    if \"```\" not in text:\n",
        "        return text\n",
        "    for p in text.split(\"```\"):\n",
        "        p = p.strip()\n",
        "        if p.lower().startswith(\"json\"):\n",
        "            p = p[4:].strip()\n",
        "        if p.startswith(\"{\"):\n",
        "            return p\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_llm_response(raw_response, raw_files):\n",
        "    text = strip_json(raw_response)\n",
        "    try:\n",
        "        out = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        return {raw: {} for raw in raw_files}\n",
        "    for raw in out:\n",
        "        for k, v in list(out[raw].items()):\n",
        "            if isinstance(v, str):\n",
        "                out[raw][k] = [v]\n",
        "    return out\n",
        "\n",
        "\n",
        "def extract_openai(manuscript_text, raw_files, prompt_spec, expected_columns, model=\"gpt-4o-mini\"):\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "    if not api_key:\n",
        "        return {raw: {} for raw in raw_files}\n",
        "    text = (manuscript_text or \"\")[:MANUSCRIPT_MAX_CHARS]\n",
        "    user = f\"MANUSCRIPT_TEXT:\\n{text}\\n\\nRAW_FILES:\\n\" + \"\\n\".join(raw_files)\n",
        "    if expected_columns:\n",
        "        user += \"\\n\\nUse these exact column names as JSON keys when applicable: \" + \", \".join(expected_columns)\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt_spec},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    raw = (resp.choices[0].message.content or \"\").strip()\n",
        "    return parse_llm_response(raw, raw_files)\n",
        "\n",
        "\n",
        "def get_manuscript(doc):\n",
        "    return \"\\n\\n\".join(doc.get(k, \"\").strip() for k in MANUSCRIPT_KEYS if doc.get(k))\n",
        "\n",
        "\n",
        "def sdrf_to_row(raw_file, sdrf_per_file, pred_columns):\n",
        "    meta = sdrf_per_file.get(raw_file, {})\n",
        "    row = {}\n",
        "    for col in pred_columns:\n",
        "        vals = meta.get(col)\n",
        "        if vals and len(vals) > 0:\n",
        "            row[col] = vals[0] if isinstance(vals, list) else str(vals)\n",
        "        else:\n",
        "            row[col] = \"Not Applicable\"\n",
        "    return row"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Load template & baseline prompt"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "prompt_spec = \"\"\n",
        "if BASELINE_PROMPT_PATH.exists():\n",
        "    prompt_spec = BASELINE_PROMPT_PATH.read_text(encoding=\"utf-8\")\n",
        "else:\n",
        "    print(\"Warning: BaselinePrompt.txt not found\")\n",
        "\n",
        "sub = pd.read_csv(SAMPLE_SUBMISSION, index_col=0)\n",
        "pred_columns = [c for c in sub.columns if c not in PREDICTION_COLUMNS_EXCLUDE]\n",
        "n_pxds = sub[\"PXD\"].nunique()\n",
        "\n",
        "use_openai = bool(os.environ.get(\"OPENAI_API_KEY\", \"\").strip()) and prompt_spec\n",
        "model = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\").strip()\n",
        "\n",
        "print(f\"Mode: {'OpenAI (' + model + ')' if use_openai else 'Placeholder'}\")\n",
        "print(f\"Template: {len(sub)} rows, {n_pxds} PXDs\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Extract per PXD & build submission"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "out_df = sub.copy()\n",
        "\n",
        "for i, (pxd, group) in enumerate(sub.groupby(\"PXD\"), start=1):\n",
        "    raw_files = group[\"Raw Data File\"].unique().tolist()\n",
        "    path = TEST_PUBTEXT / f\"{pxd}_PubText.json\"\n",
        "    if not path.exists():\n",
        "        manuscript_text = \"\"\n",
        "        print(f\"[{i}/{n_pxds}] {pxd} — no PubText\")\n",
        "    else:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            doc = json.load(f)\n",
        "        manuscript_text = get_manuscript(doc)\n",
        "        if \"Raw Data Files\" in doc:\n",
        "            raw_files = doc[\"Raw Data Files\"]\n",
        "    print(f\"[{i}/{n_pxds}] {pxd} ...\", end=\" \", flush=True)\n",
        "    sdrf_per_file = extract_openai(manuscript_text, raw_files, prompt_spec, pred_columns, model)\n",
        "    for idx, r in group.iterrows():\n",
        "        row_vals = sdrf_to_row(r[\"Raw Data File\"], sdrf_per_file, pred_columns)\n",
        "        for col in pred_columns:\n",
        "            out_df.at[idx, col] = row_vals[col]\n",
        "    print(\"ok\")\n",
        "\n",
        "out_df.to_csv(SUBMISSION_OUT, index=True)\n",
        "print(f\"\\nWrote {SUBMISSION_OUT} ({len(out_df)} rows)\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Submit to competition\n",
        "\n",
        "After **Save & Run All** completes, use **Submit** (top right) to send this notebook's output to the leaderboard. The file `submission.csv` will be in the output."
      ],
      "metadata": {}
    }
  ]
}
